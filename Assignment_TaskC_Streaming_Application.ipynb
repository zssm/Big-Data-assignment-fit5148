{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task C 2. model and streaming application\n",
    "\n",
    "# ID: 27173186 Name: Shengming Zhao\n",
    "# ID:29593387 Name:Jipeng Yin"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{\n",
    "\t'Latitude':-34.3795,\n",
    "    'Longitude':141.6331,\n",
    "    'Air Temperature (Celcius)':65,\n",
    "    'Relative Humidity':65,\n",
    "    'Windspeed (knots)':5,\n",
    "     'Max Wind Speed':8,\n",
    "     'Precipitation':'0.00I',\n",
    "     'Sender ID':0,\n",
    "     \"Created_time\" : \"08:06:30\"\n",
    "     'Hotspot': [        \n",
    "\t\t    {\n",
    "\t\t\t\"Latitude\" : \"-34.3795\",\n",
    "\t\t\t\"Longitude\" : \"141.6331\",\n",
    "\t\t\t\n",
    "\t\t\t\"Confidence\" : 65,\n",
    "            \n",
    "\t\t\t\"Surface Temperature (Celcius)\" : 62,\n",
    "            \"Created_time\" : \"08:06:30\",\n",
    "            \"Hour\" : \"8\" \n",
    "            'Sender ID':1\n",
    "            \n",
    "\t\t},\n",
    "        \n",
    "        {\n",
    "\t\t\t\"Latitude\" : \"-34.3795\",\n",
    "\t\t\t\"Longitude\" : \"141.6331\",\n",
    "\t\t\t\n",
    "\t\t\t\"Confidence\" : 65,\n",
    "            \n",
    "\t\t\t\"Surface Temperature (Celcius)\" : 62,\n",
    "            \"Created_time\" : \"08:06:30\",\n",
    "            \"Hour\" : \"8\" \n",
    "            'Sender ID':2\n",
    "\t\t}\n",
    "\t\t\n",
    "\t    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "import ast\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import geohash\n",
    "#client = MongoClient()\n",
    "#db = client.fit5148_assignment_db\n",
    "#db.drop_collection('fit5148_collection')\n",
    "def sendDataToDB(iter):\n",
    "    \n",
    "    client = MongoClient()\n",
    "    db = client.fit5148_assignment_db\n",
    "    collection = db.fit5148_collection\n",
    "    record_lst=[[],[],[]]\n",
    "    \n",
    "    for record in iter:\n",
    "        record=ast.literal_eval(record[1]) #recover its original data type and structure\n",
    "        \n",
    "        record_lst[record['Sender ID']].append(record)\n",
    "        print(record_lst[0])# add to coresponding list\n",
    "        # 0 for producer 1, 1 for producer 2, 2 for producer 3\n",
    "    \n",
    "    if len(record_lst[1])>0 and len(record_lst[2])>0: # if 2 data from TERRA and AQUA\n",
    "            # calculate hash value of 2 locations\n",
    "            hashvalue1= geohash.encode(record_lst[1][0]['Latitude'],\n",
    "                                       record_lst[1][0]['Longitude'],5)\n",
    "            \n",
    "            hashvalue2=geohash.encode(record_lst[2][0]['Latitude'],\n",
    "                                       record_lst[2][0]['Longitude'],5)\n",
    "        \n",
    "            if (hashvalue1==hashvalue2): #if 2 locations are same\n",
    "                # calculate average of temperature and confidence\n",
    "                temperature=(record_lst[2][0]['Surface Temperature (Celcius)']+\n",
    "                             record_lst[1][0]['Surface Temperature (Celcius)'])/2\n",
    "                \n",
    "                confidence=(record_lst[2][0]['Confidence']+record_lst[1][0]['Confidence'])/2\n",
    "                # assign average temperature and confidence to 2 data.\n",
    "                record_lst[1][0]['Surface Temperature (Celcius)']=temperature\n",
    "                record_lst[1][0]['Confidence']=confidence\n",
    "                record_lst[2][0]['Surface Temperature (Celcius)']=temperature\n",
    "                record_lst[2][0]['Confidence']=confidence\n",
    "                \n",
    "                \n",
    "                \n",
    "                #if  receive data from producer 2 \n",
    "    if len(record_lst[1])>0:\n",
    "        for i0 in range(len(record_lst[0])):\n",
    "            #calculate hash value for producer 1, and 2\n",
    "            hashvalue0= geohash.encode(record_lst[0][i0]['Latitude'],\n",
    "                                       record_lst[0][i0]['Longitude'],5)\n",
    "            hashvalue1=geohash.encode(record_lst[1][0]['Latitude'],\n",
    "                                       record_lst[1][0]['Longitude'],5)\n",
    "            #if same location, join them\n",
    "            if (hashvalue0==hashvalue1):\n",
    "                if 'Hotspot' in record_lst[0][i0].keys():\n",
    "                    temp=record_lst[0][i0]['Hotspot']\n",
    "                    temp.append(record_lst[1][0])\n",
    "                    \n",
    "                    record_lst[0][i0]['Hotspot']=temp\n",
    "                else:\n",
    "                    record_lst[0][i0]['Hotspot']=record_lst[1][0]\n",
    "                print(record_lst[0][i0])\n",
    "                \n",
    "                    \n",
    "                \n",
    "         #empty data from producer 2   \n",
    "        record_lst[1]=[]\n",
    "         #if  receive data from producer 3 \n",
    "    if len(record_lst[2])>0:\n",
    "        for i0 in range(len(record_lst[0])):\n",
    "            \n",
    "            hashvalue0= geohash.encode(record_lst[0][i0]['Latitude'],\n",
    "                                       record_lst[0][i0]['Longitude'],5)\n",
    "            hashvalue2=geohash.encode(record_lst[2][0]['Latitude'],\n",
    "                                       record_lst[2][0]['Longitude'],5)\n",
    "            \n",
    "            if (hashvalue0==hashvalue2):\n",
    "                if 'Hotspot' in record_lst[0][i0].keys():\n",
    "                    temp=record_lst[0][i0]['Hotspot']\n",
    "                    temp.append(record_lst[2][0])\n",
    "                    \n",
    "                    record_lst[0][i0]['Hotspot']=temp\n",
    "                else:\n",
    "                    record_lst[0][i0]['Hotspot']=record_lst[2][0]\n",
    "                print(record_lst[0][i0])\n",
    "                \n",
    "                    \n",
    "                   \n",
    "           #empty data from producer 3         \n",
    "        record_lst[2]=[]\n",
    "                    \n",
    "    for i in range(len(record_lst[0])):\n",
    "       #insert climate data to mongo db\n",
    "        try:\n",
    "            collection.insert(record_lst[0][i])\n",
    "            \n",
    "            \n",
    "        except Exception as ex:\n",
    "            print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "    \n",
    "        \n",
    "            \n",
    "                    \n",
    "    \n",
    "    client.close()\n",
    "\n",
    "n_secs = 10 # seconds of interval\n",
    "topic = \"2717318\" #topic name\n",
    "\n",
    "conf = SparkConf().setAppName(\"fit5148_taskc\").setMaster(\"local[2]\")# 2 threads\n",
    "sc = SparkContext.getOrCreate()\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, n_secs) \n",
    "    \n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092', \n",
    "                        'group.id':'27173186-group', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "                        # Group ID is completely arbitrary\n",
    "\n",
    "lines = kafkaStream.foreachRDD(lambda rdd: rdd.foreachPartition(sendDataToDB))\n",
    "\n",
    "ssc.start()\n",
    "time.sleep(600) # Run stream for 10 minutes just in case no detection of producer\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
